{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¨ Notebook 06: Interactive Gradio Demo\n",
    "\n",
    "## Objective\n",
    "Create an interactive web interface where users can upload flower images and get instant predictions.\n",
    "\n",
    "## Features\n",
    "- Drag-and-drop image upload\n",
    "- Real-time classification with ViT-Small (98.75% accuracy)\n",
    "- Top-5 predictions with confidence scores\n",
    "- Public shareable URL\n",
    "\n",
    "## Why Gradio?\n",
    "- Zero frontend code required\n",
    "- Works directly in Colab\n",
    "- Creates shareable public links\n",
    "- Professional UI out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-6.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (4.12.0)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-1.0.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==2.0.2 (from gradio)\n",
      "  Downloading gradio_client-2.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (1.2.3)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (2.2.6)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (12.0.0)\n",
      "Requirement already satisfied: pydantic<=3.0,>=2.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (2.12.5)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.21-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (6.0.3)\n",
      "Collecting safehttpx<0.2.0,>=0.1.7 (from gradio)\n",
      "  Downloading safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio) (4.15.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: fsspec in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from gradio-client==2.0.2->gradio) (2025.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ulien_hlig/flowers-cv/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gradio-6.2.0-py3-none-any.whl (23.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-2.0.2-py3-none-any.whl (55 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.128.0-py3-none-any.whl (103 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "Downloading safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.21.0-py3-none-any.whl (47 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.21-py3-none-any.whl (24 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\n",
      "Downloading ffmpy-1.0.0-py3-none-any.whl (5.6 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, brotli, uvicorn, tomlkit, semantic-version, python-multipart, orjson, mdurl, groovy, ffmpy, annotated-doc, aiofiles, starlette, markdown-it-py, safehttpx, rich, fastapi, typer, gradio-client, gradio\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20/20\u001b[0m [gradio]19/20\u001b[0m [gradio]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiofiles-24.1.0 annotated-doc-0.0.4 brotli-1.2.0 fastapi-0.128.0 ffmpy-1.0.0 gradio-6.2.0 gradio-client-2.0.2 groovy-0.1.2 markdown-it-py-4.0.0 mdurl-0.1.2 orjson-3.11.5 pydub-0.25.1 python-multipart-0.0.21 rich-14.2.0 safehttpx-0.1.7 semantic-version-2.10.0 starlette-0.50.0 tomlkit-0.13.3 typer-0.21.0 uvicorn-0.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment Setup\n",
    "# ============================================================\n",
    "# ğŸ¨ Oxford 102 Flowers - Interactive Demo\n",
    "# Bonus: Gradio Web Interface\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "This notebook creates an interactive web demo where users can:\n",
    "- Upload any flower image\n",
    "- Get instant predictions with confidence scores\n",
    "- See top-5 predictions with probabilities\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q gradio timm albumentations\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !unzip -q \"/content/drive/MyDrive/flowers-cv-submission.zip\" -d /content/ 2>/dev/null || true\n",
    "else:\n",
    "    pass  # Local environment\n",
    "    \n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Checkpoint keys: dict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias'])\n",
      "âœ… Model loaded: ViT-Small (98.75% accuracy)\n",
      "   Parameters: 21,704,934\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model\n",
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import gradio as gr\n",
    "\n",
    "# Flower class names (Oxford 102)\n",
    "CLASS_NAMES = [\n",
    "    \"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\",\n",
    "    \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\",\n",
    "    \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\",\n",
    "    \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\",\n",
    "    \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\",\n",
    "    \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\",\n",
    "    \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\",\n",
    "    \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\",\n",
    "    \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\",\n",
    "    \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\",\n",
    "    \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\",\n",
    "    \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\",\n",
    "    \"geranium\", \"orange dahlia\", \"pink-yellow dahlia\", \"cautleya spicata\",\n",
    "    \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\",\n",
    "    \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\",\n",
    "    \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\",\n",
    "    \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\",\n",
    "    \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\",\n",
    "    \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\",\n",
    "    \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\",\n",
    "    \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"\n",
    "]\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = timm.create_model(\"vit_small_patch16_224.augreg_in21k_ft_in1k\", num_classes=102)\n",
    "\n",
    "# Load weights\n",
    "if IN_COLAB:\n",
    "    MODEL_PATH = \"/content/flowers-cv/artifacts/models/vit_2stage_best.pt\"\n",
    "else:\n",
    "    MODEL_PATH = \"../artifacts/models/vit_2stage_best.pt\"\n",
    "\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "# Handle different checkpoint formats\n",
    "if isinstance(checkpoint, dict):\n",
    "    print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    elif \"state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    elif \"model\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    else:\n",
    "        # Checkpoint dict IS the state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model loaded: ViT-Small (98.75% accuracy)\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prediction function ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Prediction Function\n",
    "# Image transform (same as training)\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.CenterCrop(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def predict_flower(image):\n",
    "    \"\"\"\n",
    "    Predict flower species from an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of class names to confidence scores\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return {}\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    # Apply transforms\n",
    "    transformed = transform(image=image)\n",
    "    x = transformed[\"image\"].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "    \n",
    "    # Get top-5 predictions\n",
    "    top5_probs, top5_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    # Create output dictionary for Gradio\n",
    "    results = {\n",
    "        CLASS_NAMES[idx.item()]: prob.item() \n",
    "        for prob, idx in zip(top5_probs, top5_indices)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Prediction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gradio interface created\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Gradio Interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_flower,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Upload a Flower Image ğŸŒ¸\"),\n",
    "    outputs=gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
    "    title=\"ğŸŒ¸ Oxford 102 Flowers Classifier\",\n",
    "    description=\"\"\"\n",
    "    ## About This Model\n",
    "    - **Architecture**: Vision Transformer (ViT-Small)\n",
    "    - **Accuracy**: 98.75% on Oxford 102 Flowers test set\n",
    "    - **Training**: 2-stage fine-tuning with differential learning rates\n",
    "    \n",
    "    Upload any flower image to get instant classification!\n",
    "    \n",
    "    *Part of AQREIGHT Computer Vision Engineer Assessment*\n",
    "    \"\"\",\n",
    "    article=\"\"\"\n",
    "    ### How It Works\n",
    "    1. Upload an image of a flower\n",
    "    2. The model processes it through a Vision Transformer\n",
    "    3. Get top-5 predictions with confidence scores\n",
    "    \n",
    "    ### Model Details\n",
    "    - Trained on 102 flower species from the Oxford dataset\n",
    "    - Uses self-attention to capture global flower structure\n",
    "    - Pretrained on ImageNet-21K (14M images)\n",
    "    \n",
    "    ### Tips for Best Results\n",
    "    - Use clear, well-lit images\n",
    "    - Center the flower in the frame\n",
    "    - Single flower works better than bouquets\n",
    "    \"\"\",\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "print(\"âœ… Gradio interface created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    ğŸš€ LAUNCHING GRADIO DEMO                          â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘                                                                      â•‘\n",
      "â•‘  A public URL will be generated that anyone can access!              â•‘\n",
      "â•‘  The link is valid for 72 hours.                                     â•‘\n",
      "â•‘                                                                      â•‘\n",
      "â•‘  Features:                                                           â•‘\n",
      "â•‘  â”œâ”€ Upload any flower image                                          â•‘\n",
      "â•‘  â”œâ”€ Get instant predictions                                          â•‘\n",
      "â•‘  â”œâ”€ See top-5 classes with confidence                                â•‘\n",
      "â•‘  â””â”€ Share the link with anyone                                       â•‘\n",
      "â•‘                                                                      â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://b8e56eb6cb9b172eb5.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b8e56eb6cb9b172eb5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: Launch Demo\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸš€ LAUNCHING GRADIO DEMO                          â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  A public URL will be generated that anyone can access!              â•‘\n",
    "â•‘  The link is valid for 72 hours.                                     â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  Features:                                                           â•‘\n",
    "â•‘  â”œâ”€ Upload any flower image                                          â•‘\n",
    "â•‘  â”œâ”€ Get instant predictions                                          â•‘\n",
    "â•‘  â”œâ”€ See top-5 classes with confidence                                â•‘\n",
    "â•‘  â””â”€ Share the link with anyone                                       â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(\n",
    "    share=True,  # Creates a public URL (works in Colab!)\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    ğŸ“Š GRADIO DEMO SUMMARY                            â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘                                                                      â•‘\n",
      "â•‘  BONUS COMPLETED: Interactive Web Demo âœ…                            â•‘\n",
      "â•‘                                                                      â•‘\n",
      "â•‘  FEATURES IMPLEMENTED:                                               â•‘\n",
      "â•‘  â”œâ”€ âœ… Drag-and-drop image upload                                    â•‘\n",
      "â•‘  â”œâ”€ âœ… Real-time ViT-Small inference                                 â•‘\n",
      "â•‘  â”œâ”€ âœ… Top-5 predictions with confidence bars                        â•‘\n",
      "â•‘  â”œâ”€ âœ… Public shareable URL                                          â•‘\n",
      "â•‘  â””â”€ âœ… Professional UI with documentation                            â•‘\n",
      "â•‘                                                                      â•‘\n",
      "â•‘  WHY THIS MATTERS:                                                   â•‘\n",
      "â•‘  â”œâ”€ Stakeholders can test without coding                             â•‘\n",
      "â•‘  â”œâ”€ Easy to demonstrate model capabilities                           â•‘\n",
      "â•‘  â”œâ”€ Validates model works on real user inputs                        â•‘\n",
      "â•‘  â””â”€ Shows production-readiness mindset                               â•‘\n",
      "â•‘                                                                      â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Summary\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ“Š GRADIO DEMO SUMMARY                            â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  BONUS COMPLETED: Interactive Web Demo âœ…                            â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  FEATURES IMPLEMENTED:                                               â•‘\n",
    "â•‘  â”œâ”€ âœ… Drag-and-drop image upload                                    â•‘\n",
    "â•‘  â”œâ”€ âœ… Real-time ViT-Small inference                                 â•‘\n",
    "â•‘  â”œâ”€ âœ… Top-5 predictions with confidence bars                        â•‘\n",
    "â•‘  â”œâ”€ âœ… Public shareable URL                                          â•‘\n",
    "â•‘  â””â”€ âœ… Professional UI with documentation                            â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  WHY THIS MATTERS:                                                   â•‘\n",
    "â•‘  â”œâ”€ Stakeholders can test without coding                             â•‘\n",
    "â•‘  â”œâ”€ Easy to demonstrate model capabilities                           â•‘\n",
    "â•‘  â”œâ”€ Validates model works on real user inputs                        â•‘\n",
    "â•‘  â””â”€ Shows production-readiness mindset                               â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
